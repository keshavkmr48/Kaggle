{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_dsb_kids_game.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb626588196f4591b8f203aa258d92e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9aa77a24ad1642d7bfd23bd3b0395e78",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_25b3149496da4d119d886ca3f9b10da1",
              "IPY_MODEL_c40b6e9c44d34bac90b11d66edd5eb48"
            ]
          }
        },
        "9aa77a24ad1642d7bfd23bd3b0395e78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25b3149496da4d119d886ca3f9b10da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a70e26c122cc452bb6f01d0df77c9ccb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a112c3fa4afd41b0bf46976435fee5c9"
          }
        },
        "c40b6e9c44d34bac90b11d66edd5eb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ce8aa69dcd3e45578f489587d3cc7894",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "119it [02:37,  1.32s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1c1cd0235ab4c7d9b1154e3d9d0c5af"
          }
        },
        "a70e26c122cc452bb6f01d0df77c9ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a112c3fa4afd41b0bf46976435fee5c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce8aa69dcd3e45578f489587d3cc7894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1c1cd0235ab4c7d9b1154e3d9d0c5af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fff56aab7984c86af84908710a9ea94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f57c89e12534dfd96455199307549dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_17132b12cfa14e67950829018e73ff5f",
              "IPY_MODEL_c75649672f6e4590a44aaffde2da17d0"
            ]
          }
        },
        "0f57c89e12534dfd96455199307549dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "17132b12cfa14e67950829018e73ff5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a635d0e607a242af9bda583b6e0381a3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5905885dd2b34bf69b13baa350ad64a8"
          }
        },
        "c75649672f6e4590a44aaffde2da17d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_35bc22ec13cd4593a553c9a6a375097c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "51it [01:11,  1.39s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_928a8ea918be48139aeea88121d0e89e"
          }
        },
        "a635d0e607a242af9bda583b6e0381a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5905885dd2b34bf69b13baa350ad64a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35bc22ec13cd4593a553c9a6a375097c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "928a8ea918be48139aeea88121d0e89e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keshavkmr48/Kaggle/blob/master/NLP_DISASTER_TWEET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OEZ_beau2Bn",
        "colab_type": "text"
      },
      "source": [
        "**Setting up kaggle API to download competition dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5spsRa-6uyD5",
        "colab_type": "code",
        "outputId": "9f79ed59-e742-4dbc-e338-b840a32d7cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# !pip install kaggle --upgrade\n",
        "!pip install transformers\n",
        "\n",
        "! mkdir .kaggle\n",
        "import json\n",
        "token = {\"username\":\"keshavkmr48\",\"key\":\"014d22aa9430a95daaf028ec3e3463a2\"}\n",
        "with open('/content/.kaggle/kaggle.json','w') as file:\n",
        "  json.dump(token,file)\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "# download competition data\n",
        "!kaggle competitions download -c nlp-getting-started"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "mkdir: cannot create directory â€˜.kaggleâ€™: File exists\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading test.csv to {/content}/competitions/nlp-getting-started\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 57.0MB/s]\n",
            "Downloading train.csv to {/content}/competitions/nlp-getting-started\n",
            "  0% 0.00/965k [00:00<?, ?B/s]\n",
            "100% 965k/965k [00:00<00:00, 63.0MB/s]\n",
            "Downloading sample_submission.csv to {/content}/competitions/nlp-getting-started\n",
            "  0% 0.00/22.2k [00:00<?, ?B/s]\n",
            "100% 22.2k/22.2k [00:00<00:00, 22.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU618r3FwshF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5568f33d-25b0-4c37-838d-c613d9cb2958"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.display import display, HTML\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm_notebook\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import ensemble\n",
        "# from catboost import CatBoostRegressor\n",
        "from matplotlib import pyplot\n",
        "# import shap\n",
        "from time import time\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from collections import Counter\n",
        "from scipy import stats\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import gc, os\n",
        "import json\n",
        "import string,re\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "from wordcloud import STOPWORDS\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "import joblib\n",
        "pd.set_option('display.max_columns', 1000)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo2HHLEk32NP",
        "colab_type": "code",
        "outputId": "7277d6e4-3685-4246-a1f0-90d42b0e49f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# list data in the downloaded folder\n",
        "#os.listdir('{/content}/competitions/data-science-bowl-2019')\n",
        "os.chdir('{/content}/competitions/nlp-getting-started')\n",
        "os.listdir()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv', 'test.csv', 'sample_submission.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp1SSLMvw9yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data():\n",
        "    print('Reading train.csv file....')\n",
        "    train = pd.read_csv('train.csv')\n",
        "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
        "\n",
        "    print('Reading test.csv file....')\n",
        "    test = pd.read_csv('test.csv')\n",
        "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
        "\n",
        "\n",
        "    print('Reading sample_submission.csv file....')\n",
        "    sample_submission = pd.read_csv('sample_submission.csv')\n",
        "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
        "    return train, test, sample_submission"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQYp1vR9y0fw",
        "colab_type": "code",
        "outputId": "e7038731-d69f-4af3-e184-cac33f5872bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train, test, sample_submission = read_data()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading train.csv file....\n",
            "Training.csv file have 7613 rows and 5 columns\n",
            "Reading test.csv file....\n",
            "Test.csv file have 3263 rows and 4 columns\n",
            "Reading sample_submission.csv file....\n",
            "Sample_submission.csv file have 3263 rows and 2 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MvQIXL8wZ1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from multiprocessing import Pool\n",
        "\n",
        "# def json_loader(x):\n",
        "#   return json.loads(x)\n",
        "# pool = Pool(4)\n",
        "# result = pool.map_async(json_loader,train.event_data)\n",
        "# df = pd.DataFrame(list(result.get()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8wZoEmDy51E",
        "colab_type": "code",
        "outputId": "21ed968c-0172-4605-97c6-01cab98504c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSZlL8c2pRgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.text = train.text.astype('str')\n",
        "test.text = test.text.astype('str')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsHvjkdhMKM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slang_abbrev_dict = {\n",
        "    'AFAIK': 'As Far As I Know','AFK': 'Away From Keyboard','ASAP': 'As Soon As Possible','ATK': 'At The Keyboard','ATM': 'At The Moment','A3': 'Anytime, Anywhere, Anyplace',\n",
        "    'BAK': 'Back At Keyboard','BBL': 'Be Back Later','BBS': 'Be Back Soon','BFN': 'Bye For Now','B4N': 'Bye For Now','BRB': 'Be Right Back','BRT': 'Be Right There',\n",
        "    'BTW': 'By The Way','B4': 'Before','B4N': 'Bye For Now','CU': 'See You','CUL8R': 'See You Later','CYA': 'See You','FAQ': 'Frequently Asked Questions',\n",
        "    'FC': 'Fingers Crossed','FWIW': 'For What It\\'s Worth','FYI': 'For Your Information','GAL': 'Get A Life','GG': 'Good Game','GN': 'Good Night','GMTA': 'Great Minds Think Alike',\n",
        "    'GR8': 'Great!','G9': 'Genius','IC': 'I See','ICQ': 'I Seek you','ILU': 'I Love You','IMHO': 'In My Humble Opinion','IMO': 'In My Opinion','IOW': 'In Other Words',\n",
        "    'IRL': 'In Real Life','KISS': 'Keep It Simple, Stupid','LDR': 'Long Distance Relationship','LMAO': 'Laugh My Ass Off','LOL': 'Laughing Out Loud',\n",
        "    'LTNS': 'Long Time No See','L8R': 'Later','MTE': 'My Thoughts Exactly','M8': 'Mate','NRN': 'No Reply Necessary','OIC': 'Oh I See','OMG': 'Oh My God',\n",
        "    'PITA': 'Pain In The Ass','PRT': 'Party','PRW': 'Parents Are Watching','QPSA?': 'Que Pasa?','ROFL': 'Rolling On The Floor Laughing','ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
        "    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off','SK8': 'Skate','STATS': 'Your sex and age','ASL': 'Age, Sex, Location','THX': 'Thank You','TTFN': 'Ta-Ta For Now!',\n",
        "    'TTYL': 'Talk To You Later','U': 'You','U2': 'You Too','U4E': 'Yours For Ever','WB': 'Welcome Back','WTF': 'What The Fuck','WTG': 'Way To Go!',\n",
        "    'WUF': 'Where Are You From?','W8': 'Wait','7K': 'Sick:-D Laugher'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "649au26mMKQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mispell_dict = {\"aren't\" : \"are not\",\"can't\" : \"cannot\",\"couldn't\" : \"could not\",\"couldnt\" : \"could not\",\"didn't\" : \"did not\",\"doesn't\" : \"does not\",\n",
        "\"doesnt\" : \"does not\",\"don't\" : \"do not\",\"hadn't\" : \"had not\",\"hasn't\" : \"has not\",\"haven't\" : \"have not\",\"havent\" : \"have not\",\"he'd\" : \"he would\",\n",
        "\"he'll\" : \"he will\",\"he's\" : \"he is\",\"i'd\" : \"I would\",\"i'd\" : \"I had\",\"i'll\" : \"I will\",\"i'm\" : \"I am\",\"isn't\" : \"is not\",\"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\",\"i've\" : \"I have\",\"let's\" : \"let us\",\"mightn't\" : \"might not\",\"mustn't\" : \"must not\",\"shan't\" : \"shall not\",\"she'd\" : \"she would\",\n",
        "\"she'll\" : \"she will\",\"she's\" : \"she is\",\"shouldn't\" : \"should not\",\"shouldnt\" : \"should not\",\"that's\" : \"that is\",\"thats\" : \"that is\",\"there's\" : \"there is\",\n",
        "\"theres\" : \"there is\",\"they'd\" : \"they would\",\"they'll\" : \"they will\",\"they're\" : \"they are\",\"theyre\":  \"they are\",\"they've\" : \"they have\",\"we'd\" : \"we would\",\n",
        "\"we're\" : \"we are\",\"weren't\" : \"were not\",\"we've\" : \"we have\",\"what'll\" : \"what will\",\"what're\" : \"what are\",\"what's\" : \"what is\",\"what've\" : \"what have\",\n",
        "\"where's\" : \"where is\",\"who'd\" : \"who would\",\"who'll\" : \"who will\",\"who're\" : \"who are\",\"who's\" : \"who is\",\"who've\" : \"who have\",\"won't\" : \"will not\",\n",
        "\"wouldn't\" : \"would not\",\"you'd\" : \"you would\",\"you'll\" : \"you will\",\"you're\" : \"you are\",\"you've\" : \"you have\",\"'re\": \" are\",\"wasn't\": \"was not\",\n",
        "\"we'll\":\" will\",\"didn't\": \"did not\",\"tryin'\":\"trying\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfMJ7diXMKWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    add_stopwords=[\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\", \"am\",\"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\", \"because\",\n",
        "    \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\",\"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\", \"do\", \"does\", \"doesn\",\"doesn't\", \n",
        "    \"doing\", \"don\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\",\"from\", \"further\", \"had\", \"hadn\", \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\",\n",
        "    \"haven\", \"haven't\", \"having\", \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\",\"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"isn\", \"isn't\",\n",
        "    \"it\", \"it's\", \"its\", \"itself\", \"just\", \"ll\", \"m\", \"ma\", \"me\", \"mightn\",\"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"needn\",\n",
        "    \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\",\"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
        "    \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should\", \"should've\",\"shouldn\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\", \"that\",\n",
        "    \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\",\"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\",\n",
        "    \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\",\"weren't\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\",\n",
        "    \"will\", \"with\", \"won\", \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\",\"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\",\n",
        "    \"could\", \"he'd\", \"he'll\", \"he's\", \"here's\", \"how's\", \"i'd\", \"i'll\", \"i'm\",\"i've\", \"let's\", \"ought\", \"she'd\", \"she'll\", \"that's\", \"there's\", \"they'd\",\n",
        "    \"they'll\", \"they're\", \"they've\", \"we'd\", \"we'll\", \"we're\", \"we've\",\"what's\", \"when's\", \"where's\", \"who's\", \"why's\", \"would\", \"able\", \"abst\",\n",
        "    \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\",\"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"afterwards\", \"ah\",\n",
        "    \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\"among\", \"amongst\", \"announce\", \"another\", \"anybody\", \"anyhow\", \"anymore\",\n",
        "    \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\",\"approximately\", \"arent\", \"arise\", \"around\", \"aside\", \"ask\", \"asking\",\n",
        "    \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"became\", \"become\",\"becomes\", \"becoming\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\",\n",
        "    \"begins\", \"behind\", \"believe\", \"beside\", \"besides\", \"beyond\", \"biol\",\"brief\", \"briefly\", \"c\", \"ca\", \"came\", \"cannot\", \"can't\", \"cause\", \"causes\",\n",
        "    \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\",\"containing\", \"contains\", \"couldnt\", \"date\", \"different\", \"done\",\n",
        "    \"downwards\", \"due\", \"e\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\",\"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\",\n",
        "    \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"ff\", \"fifth\", \"first\", \"five\",\n",
        "    \"fix\", \"followed\", \"following\", \"follows\", \"former\", \"formerly\", \"forth\",\"found\", \"four\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\",\n",
        "    \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\",\"h\", \"happens\", \"hardly\", \"hed\", \"hence\", \"hereafter\", \"hereby\", \"herein\",\n",
        "    \"heres\", \"hereupon\", \"hes\", \"hi\", \"hid\", \"hither\", \"home\", \"howbeit\",\"however\", \"hundred\", \"id\", \"ie\", \"im\", \"immediate\", \"immediately\",\n",
        "    \"importance\", \"important\", \"inc\", \"indeed\", \"index\", \"information\",\"instead\", \"invention\", \"inward\", \"itd\", \"it'll\", \"j\", \"k\", \"keep\", \"keeps\",\n",
        "    \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\",\"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\",\n",
        "    \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\",\"looking\", \"looks\", \"ltd\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\",\n",
        "    \"maybe\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\",\"million\", \"miss\", \"ml\", \"moreover\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\",\n",
        "    \"must\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\",\"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n",
        "    \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"nobody\", \"non\", \"none\",\"nonetheless\", \"noone\", \"normally\", \"nos\", \"noted\", \"nothing\", \"nowhere\",\n",
        "    \"obtain\", \"obtained\", \"obviously\", \"often\", \"oh\", \"ok\", \"okay\", \"old\",\"omitted\", \"one\", \"ones\", \"onto\", \"ord\", \"others\", \"otherwise\", \"outside\",\n",
        "    \"overall\", \"owing\", \"p\", \"page\", \"pages\", \"part\", \"particular\",\"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\",\n",
        "    \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\",\"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\",\n",
        "    \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\",\"rather\", \"rd\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\",\n",
        "    \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\",\"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"said\",\n",
        "    \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\",\"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\",\n",
        "    \"several\", \"shall\", \"shed\", \"shes\", \"show\", \"showed\", \"shown\", \"showns\",\"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\",\n",
        "    \"six\", \"slightly\", \"somebody\", \"somehow\", \"someone\", \"somethan\",\"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n",
        "    \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\",\"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"sufficiently\",\n",
        "    \"suggest\", \"sup\", \"sure\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\",\"thank\", \"thanks\", \"thanx\", \"thats\", \"that've\", \"thence\", \"thereafter\",\n",
        "    \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\",\"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"theyd\", \"theyre\",\n",
        "    \"think\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"throughout\",\"thru\", \"thus\", \"til\", \"tip\", \"together\", \"took\", \"toward\", \"towards\",\n",
        "    \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\",\"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"unto\", \"upon\", \"ups\",\n",
        "    \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\",\"usually\", \"v\", \"value\", \"various\", \"'ve\", \"via\", \"viz\", \"vol\", \"vols\",\n",
        "    \"vs\", \"w\", \"want\", \"wants\", \"wasnt\", \"way\", \"wed\", \"welcome\", \"went\",\"werent\", \"whatever\", \"what'll\", \"whats\", \"whence\", \"whenever\",\n",
        "    \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\",\"wherever\", \"whether\", \"whim\", \"whither\", \"whod\", \"whoever\", \"whole\",\n",
        "    \"who'll\", \"whomever\", \"whos\", \"whose\", \"widely\", \"willing\", \"wish\",\"within\", \"without\", \"wont\", \"words\", \"world\", \"wouldnt\", \"www\", \"x\", \"yes\",\n",
        "    \"yet\", \"youd\", \"youre\", \"z\", \"zero\", \"a's\", \"ain't\", \"allow\", \"allows\",\"apart\", \"appear\", \"appreciate\", \"appropriate\", \"associated\", \"best\",\n",
        "    \"better\", \"c'mon\", \"c's\", \"cant\", \"changes\", \"clearly\", \"concerning\",\"consequently\", \"consider\", \"considering\", \"corresponding\", \"course\",\n",
        "    \"currently\", \"definitely\", \"described\", \"despite\", \"entirely\", \"exactly\",\"example\", \"going\", \"greetings\", \"hello\", \"help\", \"hopefully\", \"ignored\",\n",
        "    \"inasmuch\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\",\"it'd\", \"keep\", \"keeps\", \"novel\", \"presumably\", \"reasonably\", \"second\",\n",
        "    \"secondly\", \"sensible\", \"serious\", \"seriously\", \"sure\", \"t's\", \"third\",\"thorough\", \"thoroughly\", \"three\", \"well\", \"wonder\", \"a\", \"about\", \"above\",\n",
        "    \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\",\"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\",\n",
        "    \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\",\n",
        "    \"at\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\",\"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\n",
        "    \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\",\"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\",\n",
        "    \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\",\"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\",\n",
        "    \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\",\"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\",\n",
        "    \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\",\"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\",\n",
        "    \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\",\"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\",\n",
        "    \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\",\"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\",\n",
        "    \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\",\"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\",\n",
        "    \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\",\n",
        "    \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\",\"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
        "    \"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\",\"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\",\n",
        "    \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\",\n",
        "    \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
        "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\",\"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
        "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
        "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
        "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\",\n",
        "    \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\"the\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
        "    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"A\", \"B\", \"C\",\"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\",\n",
        "    \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"co\", \"op\", \"research-articl\",\"pagecount\", \"cit\", \"ibid\", \"les\", \"le\", \"au\", \"que\", \"est\", \"pas\", \"vol\",\n",
        "    \"el\", \"los\", \"pp\", \"u201d\", \"well-b\", \"http\", \"volumtype\", \"par\", \"0o\",\"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"ac\",\n",
        "    \"ad\", \"ae\", \"af\", \"ag\", \"aj\", \"al\", \"an\", \"ao\", \"ap\", \"ar\", \"av\", \"aw\",\"ax\", \"ay\", \"az\", \"b1\", \"b2\", \"b3\", \"ba\", \"bc\", \"bd\", \"be\", \"bi\", \"bj\",\n",
        "    \"bk\", \"bl\", \"bn\", \"bp\", \"br\", \"bs\", \"bt\", \"bu\", \"bx\", \"c1\", \"c2\", \"c3\",\"cc\", \"cd\", \"ce\", \"cf\", \"cg\", \"ch\", \"ci\", \"cj\", \"cl\", \"cm\", \"cn\", \"cp\",\n",
        "    \"cq\", \"cr\", \"cs\", \"ct\", \"cu\", \"cv\", \"cx\", \"cy\", \"cz\", \"d2\", \"da\", \"dc\",\"dd\", \"de\", \"df\", \"di\", \"dj\", \"dk\", \"dl\", \"do\", \"dp\", \"dr\", \"ds\", \"dt\",\n",
        "    \"du\", \"dx\", \"dy\", \"e2\", \"e3\", \"ea\", \"ec\", \"ed\", \"ee\", \"ef\", \"ei\", \"ej\",\"el\", \"em\", \"en\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"et\", \"eu\", \"ev\", \"ex\",\n",
        "    \"ey\", \"f2\", \"fa\", \"fc\", \"ff\", \"fi\", \"fj\", \"fl\", \"fn\", \"fo\", \"fr\", \"fs\", \"ft\", \"fu\", \"fy\", \"ga\", \"ge\", \"gi\", \"gj\", \"gl\", \"go\", \"gr\", \"gs\", \"gy\",\n",
        "    \"h2\", \"h3\", \"hh\", \"hi\", \"hj\", \"ho\", \"hr\", \"hs\", \"hu\", \"hy\", \"i\", \"i2\", \"i3\",\"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ic\", \"ie\", \"ig\", \"ih\", \"ii\", \"ij\",\"il\", \"in\", \"io\", \"ip\", \"iq\", \"ir\", \"iv\", \"ix\", \"iy\", \"iz\", \"jj\", \"jr\",\n",
        "    \"js\", \"jt\", \"ju\", \"ke\", \"kg\", \"kj\", \"km\", \"ko\", \"l2\", \"la\", \"lb\", \"lc\",\"lf\", \"lj\", \"ln\", \"lo\", \"lr\", \"ls\", \"lt\", \"m2\", \"ml\", \"mn\", \"mo\", \"ms\",\n",
        "    \"mt\", \"mu\", \"n2\", \"nc\", \"nd\", \"ne\", \"ng\", \"ni\", \"nj\", \"nl\", \"nn\", \"nr\",\"ns\", \"nt\", \"ny\", \"oa\", \"ob\", \"oc\", \"od\", \"of\", \"og\", \"oi\", \"oj\", \"ol\",\n",
        "    \"om\", \"on\", \"oo\", \"oq\", \"or\", \"os\", \"ot\", \"ou\", \"ow\", \"ox\", \"oz\", \"p1\",\"p2\", \"p3\", \"pc\", \"pd\", \"pe\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"pm\",\n",
        "    \"pn\", \"po\", \"pq\", \"pr\", \"ps\", \"pt\", \"pu\", \"py\", \"qj\", \"qu\", \"r2\", \"ra\",\"rc\", \"rd\", \"rf\", \"rh\", \"ri\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\",\n",
        "    \"rs\", \"rt\", \"ru\", \"rv\", \"ry\", \"s2\", \"sa\", \"sc\", \"sd\", \"se\", \"sf\", \"si\",\"sj\", \"sl\", \"sm\", \"sn\", \"sp\", \"sq\", \"sr\", \"ss\", \"st\", \"sy\", \"sz\", \"t1\",\n",
        "    \"t2\", \"t3\", \"tb\", \"tc\", \"td\", \"te\", \"tf\", \"th\", \"ti\", \"tj\", \"tl\", \"tm\",\"tn\", \"tp\", \"tq\", \"tr\", \"ts\", \"tt\", \"tv\", \"tx\", \"ue\", \"ui\", \"uj\", \"uk\",\n",
        "    \"um\", \"un\", \"uo\", \"ur\", \"ut\", \"va\", \"wa\", \"vd\", \"wi\", \"vj\", \"vo\", \"wo\",\"vq\", \"vt\", \"vu\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n",
        "    \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y2\", \"yj\", \"yl\", \"yr\", \"ys\", \"yt\", \"zi\", \"zz\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sphwz-wuMKa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb-gYDwpMKfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_numbers(x):\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZslfHGum0jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(x):\n",
        "    x = str(x).replace(\"\\n\",\"\")\n",
        "    \n",
        "    stops  = set(list(STOPWORDS)+add_stopwords)\n",
        "    text = [w for w in word_tokenize(x) if w not in stops]    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h76fZR5BMKk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoq0_1UKMKqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_typical_misspell(text):\n",
        "    mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
        "\n",
        "    def replace(match):\n",
        "        return mispellings[match.group(0)]\n",
        "\n",
        "    return mispellings_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFrMZVoqMKop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unslang(text):\n",
        "    \"\"\"Converts text like \"OMG\" into \"Oh my God\"\n",
        "    \"\"\"\n",
        "    text = [slang_abbrev_dict[w.upper()] if w.upper() in slang_abbrev_dict.keys() else w for w in word_tokenize(text)]    \n",
        "    return \" \".join(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0PpxyzqMKih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIeBLHIFMKd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMdSkuBPlDEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html(text):\n",
        "    return BeautifulSoup(text, \"lxml\").text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMYoFAyClDOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_data(df, col):\n",
        "        df[col] = df[col].apply(lambda x: clean_numbers(x))\n",
        "        df[col] = df[col].apply(lambda x: remove_urls(x))\n",
        "        df[col] = df[col].apply(lambda x: remove_html(x))\n",
        "        df[col] = df[col].apply(lambda text: remove_punctuation(text))\n",
        "        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n",
        "        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n",
        "        df[col] = df[col].apply(lambda x: unslang(x))\n",
        "        df[col] = df[col].apply(lambda x: remove_emoji(x))\n",
        "        return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1m9iEi4lDWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = clean_data(train, 'text')\n",
        "test = clean_data(test, 'text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHuA4VhTxyzW",
        "colab_type": "code",
        "outputId": "f9184d1d-46e7-4055-a299-21c3b089e17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train[\"kfold\"] = -1\n",
        "\n",
        "    train = train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n",
        "\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(X=train, y=train.target.values)):\n",
        "        print(len(train_idx), len(val_idx))\n",
        "        train.loc[val_idx, 'kfold'] = fold\n",
        "    \n",
        "\n",
        "    train.to_csv(\"train_folds.csv\", index=False)\n",
        "del train\n",
        "TRAINING_DATA = pd.read_csv('train_folds.csv')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6089 1524\n",
            "6090 1523\n",
            "6091 1522\n",
            "6091 1522\n",
            "6091 1522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSqjWr0CoUC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06069fd6-0e75-4cad-a5f4-bd4e80a0d78d"
      },
      "source": [
        "# feature_engineering on text column\n",
        "\n",
        "# char_count\n",
        "TRAINING_DATA['char_count'] = TRAINING_DATA['text'].apply(lambda x: len(str(x)))\n",
        "test['char_count'] = test['text'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# punctuation_count\n",
        "TRAINING_DATA['punctuation_count'] = TRAINING_DATA['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "test['punctuation_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "# hashtag_count\n",
        "TRAINING_DATA['hashtag_count'] = TRAINING_DATA['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "test['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# mention_count\n",
        "TRAINING_DATA['mention_count'] = TRAINING_DATA['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "test['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "\n",
        "# unique_word_count\n",
        "TRAINING_DATA['unique_word_count'] = TRAINING_DATA['text'].apply(lambda x: len(set(str(x).split())))\n",
        "test['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "# stop_word_count\n",
        "TRAINING_DATA['stop_word_count'] = TRAINING_DATA['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
        "test['stop_word_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
        "\n",
        "# url_count\n",
        "TRAINING_DATA['url_count'] = TRAINING_DATA['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "test['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "# mean_word_length\n",
        "TRAINING_DATA['mean_word_length'] = TRAINING_DATA['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test['mean_word_length'] = test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "# word_count\n",
        "TRAINING_DATA['word_count'] = TRAINING_DATA['text'].apply(lambda x: len(str(x).split()))\n",
        "test['word_count'] = test['text'].apply(lambda x: len(str(x).split()))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tIE8msh9uXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Missing value filler \n",
        "TRAINING_DATA.keyword = TRAINING_DATA.keyword.fillna('No_Location')\n",
        "TRAINING_DATA.location = TRAINING_DATA.location.fillna('No_Location')\n",
        "test.keyword = test.keyword.fillna('No_Key')\n",
        "test.location = test.location.fillna('No_Location')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QonmqDKTFto7",
        "colab_type": "code",
        "outputId": "ccbef41a-52a4-4705-f358-250c75df8854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "TRAINING_DATA.head(3)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>kfold</th>\n",
              "      <th>char_count</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>hashtag_count</th>\n",
              "      <th>mention_count</th>\n",
              "      <th>unique_word_count</th>\n",
              "      <th>stop_word_count</th>\n",
              "      <th>url_count</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9485</td>\n",
              "      <td>terrorism</td>\n",
              "      <td>No_Location</td>\n",
              "      <td>articles saudi press reject russian initiative...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.833333</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8955</td>\n",
              "      <td>storm</td>\n",
              "      <td>nor*cal</td>\n",
              "      <td>tonyhsieh person dances rain walk storm anonymous</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.142857</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1525</td>\n",
              "      <td>body%20bags</td>\n",
              "      <td>No_Location</td>\n",
              "      <td>zicac vintage leather briefcase messenger satc...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.454545</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id      keyword     location  \\\n",
              "0  9485    terrorism  No_Location   \n",
              "1  8955        storm      nor*cal   \n",
              "2  1525  body%20bags  No_Location   \n",
              "\n",
              "                                                text  target  kfold  \\\n",
              "0  articles saudi press reject russian initiative...       0      0   \n",
              "1  tonyhsieh person dances rain walk storm anonymous       0      0   \n",
              "2  zicac vintage leather briefcase messenger satc...       0      0   \n",
              "\n",
              "   char_count  punctuation_count  hashtag_count  mention_count  \\\n",
              "0          93                  0              0              0   \n",
              "1          49                  0              0              0   \n",
              "2          81                  0              0              0   \n",
              "\n",
              "   unique_word_count  stop_word_count  url_count  mean_word_length  word_count  \n",
              "0                 12                0          0          6.833333          12  \n",
              "1                  7                0          0          6.142857           7  \n",
              "2                 11                0          0          6.454545          11  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeKz7_ehCK2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAINING_DATA=TRAINING_DATA[-TRAINING_DATA.text.isnull()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0tnaMgMFtsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# n-gram generator\n",
        "def generate_ngrams(text, n_gram=1):\n",
        "  try:\n",
        "    token = [token for token in text.split(' ') if token != '' if token not in STOPWORDS]\n",
        "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
        "    return [' '.join(ngram) for ngram in ngrams]\n",
        "  except:\n",
        "    print(text)\n",
        "\n",
        "def n_gram_distribution_generator(n_gram=1):\n",
        "  disaster_ngrams = defaultdict(int)\n",
        "  nondisaster_ngrams = defaultdict(int)\n",
        "\n",
        "  for tweet in TRAINING_DATA.text[TRAINING_DATA.target==1]:\n",
        "      for word in generate_ngrams(tweet,n_gram=n_gram):\n",
        "          disaster_ngrams[word] += 1\n",
        "\n",
        "  for tweet in TRAINING_DATA.text[TRAINING_DATA.target==0]:\n",
        "      for word in generate_ngrams(tweet,n_gram=n_gram):\n",
        "          nondisaster_ngrams[word] += 1\n",
        "  df_disaster_ngrams= pd.DataFrame(disaster_ngrams.items(), columns=['word','count']).sort_values(by='count', ascending=False)\n",
        "  df_non_disaster_ngrams = pd.DataFrame(nondisaster_ngrams.items(), columns=['word','count']).sort_values(by='count', ascending=False)\n",
        "  return df_disaster_ngrams, df_non_disaster_ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1RgtzAeaw79",
        "colab_type": "text"
      },
      "source": [
        "UNIGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R0ZmJG2YjGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "196f99c9-596e-43ae-a1ec-fb1bd2a0e302"
      },
      "source": [
        "df_disaster_ngrams, df_non_disaster_ngrams = n_gram_distribution_generator(n_gram=1)\n",
        "print(f'frequency count of unigram for disaster tweets \\n {df_disaster_ngrams.head()}');\n",
        "print( f'frequency count of unigram for non-disaster tweets \\n {df_non_disaster_ngrams.head()}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency count of unigram for disaster tweets \n",
            "            word  count\n",
            "352        news    136\n",
            "173    disaster    117\n",
            "253  california    111\n",
            "84      suicide    110\n",
            "278      police    106\n",
            "frequency count of unigram for non-disaster tweets \n",
            "         word  count\n",
            "91      dont    139\n",
            "27      body    112\n",
            "461    video     96\n",
            "2058  people     92\n",
            "101        2     92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA5DbShzeCam",
        "colab_type": "text"
      },
      "source": [
        "BIGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4zNCYcMbAnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "00e9379d-fd1b-4d2c-b2a3-13b6e1ca8491"
      },
      "source": [
        "df_disaster_ngrams, df_non_disaster_ngrams = n_gram_distribution_generator(n_gram=2)\n",
        "print(f'frequency count of bigram for disaster tweets \\n {df_disaster_ngrams.head()}');\n",
        "print( f'frequency count of bigram for non-disaster tweets \\n {df_non_disaster_ngrams.head()}')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency count of bigram for disaster tweets \n",
            "                      word  count\n",
            "76         suicide bomber     59\n",
            "685   northern california     41\n",
            "1575            oil spill     38\n",
            "87      burning buildings     35\n",
            "1571  california wildfire     34\n",
            "frequency count of bigram for non-disaster tweets \n",
            "               word  count\n",
            "234       Out Loud     60\n",
            "233   Laughing Out     60\n",
            "24      cross body     38\n",
            "644  youtube video     36\n",
            "771       body bag     26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VfestM5eRZG",
        "colab_type": "text"
      },
      "source": [
        "TRIGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BDsYXh0d-Vw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1d258e6e-22e9-4936-9487-1d5c5f33181a"
      },
      "source": [
        "df_disaster_ngrams, df_non_disaster_ngrams = n_gram_distribution_generator(n_gram=3)\n",
        "print(f'frequency count of trigram for disaster tweets \\n {df_disaster_ngrams.head()}');\n",
        "print( f'frequency count of trigram for non-disaster tweets \\n {df_non_disaster_ngrams.head()}')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frequency count of trigram for disaster tweets \n",
            "                               word  count\n",
            "65        suicide bomber detonated     30\n",
            "1388  northern california wildfire     29\n",
            "1385            latest homes razed     28\n",
            "1386          homes razed northern     28\n",
            "66           bomber detonated bomb     28\n",
            "frequency count of trigram for non-disaster tweets \n",
            "                               word  count\n",
            "200              Laughing Out Loud     60\n",
            "715                 cross body bag     18\n",
            "580   quarantine offensive content     18\n",
            "579    reddit quarantine offensive     18\n",
            "1755                 pick fan army     17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsok8jJtlDUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "04086107-8394-47e8-f08d-d3e9d63c8114"
      },
      "source": [
        "TRAINING_DATA.text.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    articles saudi press reject russian initiative...\n",
              "1    tonyhsieh person dances rain walk storm anonymous\n",
              "2    zicac vintage leather briefcase messenger satc...\n",
              "3                                            eileenmfl\n",
              "4    tomorrow kick weekend drinks entertainment ali...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5DgIjS8lDJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunks(l, n):\n",
        "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i + n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUKD5RdnlDIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fetch_vectors(string_list, batch_size=64):\n",
        "    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    \n",
        "    # For DistilBERT:\n",
        "    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "    \n",
        "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    model = model_class.from_pretrained(pretrained_weights)\n",
        "    \n",
        "    model.to(DEVICE)\n",
        "\n",
        "    fin_features = []\n",
        "    for data in tqdm_notebook(chunks(string_list, batch_size)):\n",
        "        tokenized = []\n",
        "        for x in data:\n",
        "            x = \" \".join(x.strip().split()[:200])\n",
        "            tok = tokenizer.encode(x, add_special_tokens=True)\n",
        "            tokenized.append(tok[:512])\n",
        "\n",
        "        max_len = 512\n",
        "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n",
        "        attention_mask = np.where(padded != 0, 1, 0)\n",
        "        input_ids = torch.tensor(padded).to(DEVICE)\n",
        "        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
        "        fin_features.append(features)\n",
        "\n",
        "    fin_features = np.vstack(fin_features)\n",
        "    return fin_features,tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W1yqDXmF5Df",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "fb626588196f4591b8f203aa258d92e7",
            "9aa77a24ad1642d7bfd23bd3b0395e78",
            "25b3149496da4d119d886ca3f9b10da1",
            "c40b6e9c44d34bac90b11d66edd5eb48",
            "a70e26c122cc452bb6f01d0df77c9ccb",
            "a112c3fa4afd41b0bf46976435fee5c9",
            "ce8aa69dcd3e45578f489587d3cc7894",
            "e1c1cd0235ab4c7d9b1154e3d9d0c5af",
            "0fff56aab7984c86af84908710a9ea94",
            "0f57c89e12534dfd96455199307549dd",
            "17132b12cfa14e67950829018e73ff5f",
            "c75649672f6e4590a44aaffde2da17d0",
            "a635d0e607a242af9bda583b6e0381a3",
            "5905885dd2b34bf69b13baa350ad64a8",
            "35bc22ec13cd4593a553c9a6a375097c",
            "928a8ea918be48139aeea88121d0e89e"
          ]
        },
        "outputId": "bca13feb-eda7-4cb6-cb48-eea0a5352a26"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "train_dense,traintokenizer = fetch_vectors(TRAINING_DATA.text.values)\n",
        "test_dense,testtokenizer = fetch_vectors(test.text.values)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb626588196f4591b8f203aa258d92e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fff56aab7984c86af84908710a9ea94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ouUHQSF5Id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDHkAoHQF5OV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LaDq7mmF5Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qemoNrxlF5G_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1PTguR-MKY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2vec embedding extraction\n",
        "def text_processor (text_value):\n",
        "  text = 'ram'\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W86cn6tbJHuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODELS = {\n",
        "    \"randomforest\": ensemble.RandomForestClassifier(n_estimators=200, n_jobs=-1, verbose=2),\n",
        "    \"extratrees\": ensemble.ExtraTreesClassifier(n_estimators=200, n_jobs=-1, verbose=2),\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGRSvczd-gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FOLD_MAPPPING = {\n",
        "    0: [1, 2, 3, 4],\n",
        "    1: [0, 2, 3, 4],\n",
        "    2: [0, 1, 3, 4],\n",
        "    3: [0, 1, 2, 4],\n",
        "    4: [0, 1, 2, 3]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C1NPyfN6FMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  for MODEL in MODELS.keys():\n",
        "    for FOLD in range(5):\n",
        "      train_df = TRAINING_DATA[TRAINING_DATA.kfold.isin(FOLD_MAPPPING.get(FOLD))].reset_index(drop=True)\n",
        "      valid_df = TRAINING_DATA[TRAINING_DATA.kfold==FOLD].reset_index(drop=True)\n",
        "\n",
        "      ytrain = train_df.target.values\n",
        "      yvalid = valid_df.target.values\n",
        "\n",
        "      train_df = train_df.drop([\"id\", \"target\", \"kfold\"], axis=1)\n",
        "      valid_df = valid_df.drop([\"id\", \"target\", \"kfold\"], axis=1)\n",
        "\n",
        "      train_df.text = train_df.text.apply(lambda x : text_processor(x))\n",
        "      valid_df.text = valid_df.text.apply(lambda x : text_processor(x))\n",
        "\n",
        "      valid_df = valid_df[train_df.columns]\n",
        "\n",
        "      # label_encoders = {}\n",
        "      # for c in train_df.columns:\n",
        "      #     lbl = preprocessing.LabelEncoder()\n",
        "      #     lbl.fit(train_df[c].values.tolist() + valid_df[c].values.tolist() + df_test[c].values.tolist())\n",
        "      #     train_df.loc[:, c] = lbl.transform(train_df[c].values.tolist())\n",
        "      #     valid_df.loc[:, c] = lbl.transform(valid_df[c].values.tolist())\n",
        "      #     label_encoders[c] = lbl\n",
        "      \n",
        "      # data is ready to train\n",
        "      clf = MODELS[MODEL]\n",
        "      clf.fit(train_df, ytrain)\n",
        "      preds = clf.predict_proba(valid_df)[:, 1]\n",
        "      print(metrics.roc_auc_score(yvalid, preds))\n",
        "\n",
        "      # joblib.dump(label_encoders, f\"models/{MODEL}_{FOLD}_label_encoder.pkl\")\n",
        "      joblib.dump(clf, f\"models/{MODEL}_{FOLD}.pkl\")\n",
        "      joblib.dump(train_df.columns, f\"models/{MODEL}_{FOLD}_columns.pkl\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}